{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m460.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /home/pirapuraj/.local/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m678.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.16.0-py3-none-any.whl (16 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/pirapuraj/.local/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.0.0\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.13.2-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch) (2.0.1)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed filelock-3.16.0 fsspec-2024.9.0 mpmath-1.3.0 networkx-3.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 sympy-1.13.2 torch-2.4.1 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, List, Optional, Dict\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from torch import nn\n",
    "import random\n",
    "import torch\n",
    "import platform\n",
    "from typing import Callable, List, Optional, Dict\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(platform.python_version())\n",
    "\n",
    "# from torch_geometric.data import (\n",
    "# #     HeteroData,\n",
    "#     Data,\n",
    "#     InMemoryDataset,\n",
    "#     Batch\n",
    "#     )\n",
    "# from torch_geometric.data.storage import EdgeStorage\n",
    "# import torch_geometric.datasets as datasets\n",
    "# import torch_geometric.transforms as transforms\n",
    "# from torch_geometric.data import Data\n",
    "# from torch_geometric.loader import DataLoader\n",
    "\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"/raid/cs21mtech12001/API-Misuse-Research/PDG-Gen/Repository/Processed Dataset/After pruning/new/PreparedStatement.executeQuery/\", \n",
    "           \"/raid/cs21mtech12001/API-Misuse-Research/PDG-Gen/Repository/Processed Dataset/After pruning/new/Calendar.getTime/\",\n",
    "           \"/raid/cs21mtech12001/API-Misuse-Research/PDG-Gen/Repository/Processed Dataset/After pruning/new/URL.openConnection/\",\n",
    "           \"/raid/cs21mtech12001/API-Misuse-Research/PDG-Gen/Repository/Processed Dataset/After pruning/new/BufferedReader.readLine/\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to process raw graph data(in .txt format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_edges(inTextFile, add_reverse_edges = False):\n",
    "  # FD = 0, CD = 1\n",
    "  # to support the hetero data object as suggested by the documentation \n",
    "  nodes_dict = {}\n",
    "  edge_indices_CD = []\n",
    "  edge_indices_FD = []\n",
    "\n",
    "  #to support the Data object as used by the Entities dat object as used in RGAT source code\n",
    "  edge_indices = []\n",
    "  edge_type = []\n",
    "  \n",
    "  # nodes_dict is an index_map\n",
    "  node_count=0\n",
    "  with open(inTextFile) as fp:\n",
    "    \n",
    "    file_name = inTextFile.split(\"/\")[-1].strip()\n",
    "\n",
    "    Lines = fp.readlines()\n",
    "    for line in Lines:\n",
    "\n",
    "      N = line.split('-->')\n",
    "      N[0], N[1] = N[0].strip(), N[1].strip()\n",
    "      \n",
    "      #t1 = N[0].split('$$')   \n",
    "      src = N[0].strip()   \n",
    "      if src not in nodes_dict.keys():\n",
    "        nodes_dict[src] = node_count\n",
    "        node_count+=1\n",
    "        \n",
    "      #t2 = N[1].split('$$')\n",
    "      right_idx = N[1].rfind('[')\n",
    "      dst = N[1][:right_idx].strip()\n",
    "      if dst not in nodes_dict.keys():\n",
    "        nodes_dict[dst] = node_count\n",
    "        node_count+=1\n",
    "\n",
    "      x = N[1].strip()[right_idx + 1 : -1].strip()\n",
    "      if(x == 'FD'):\n",
    "        y=0\n",
    "        edge_type.append(y)\n",
    "        edge_indices.append([nodes_dict[src], nodes_dict[dst]])\n",
    "        if add_reverse_edges:\n",
    "          edge_type.append(y)\n",
    "          edge_indices.append([nodes_dict[dst], nodes_dict[src]])\n",
    "        edge_indices_FD.append([nodes_dict[src], nodes_dict[dst]])\n",
    "      else: \n",
    "        y=1\n",
    "        edge_type.append(y)\n",
    "        edge_indices.append([nodes_dict[src], nodes_dict[dst]])\n",
    "        if add_reverse_edges:\n",
    "          edge_type.append(y)\n",
    "          edge_indices.append([nodes_dict[dst], nodes_dict[src]])\n",
    "        edge_indices_CD.append([nodes_dict[src], nodes_dict[dst]])\n",
    "     \n",
    "  return nodes_dict, edge_indices_FD, edge_indices_CD, edge_indices, edge_type, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/raid/cs21mtech12001/API-Misuse-Research/PDG-Gen/Repository/Processed Dataset/After pruning/new/BufferedReader.readLine/0_sample-1_BufferedReader.readLine_graph_dump.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-aded392bc728>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/raid/cs21mtech12001/API-Misuse-Research/PDG-Gen/Repository/Processed Dataset/After pruning/new/BufferedReader.readLine/0_sample-1_BufferedReader.readLine_graph_dump.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_nodes_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_reverse_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-106e6ef8312b>\u001b[0m in \u001b[0;36mget_nodes_edges\u001b[0;34m(inTextFile, add_reverse_edges)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# nodes_dict is an index_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mnode_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minTextFile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minTextFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/raid/cs21mtech12001/API-Misuse-Research/PDG-Gen/Repository/Processed Dataset/After pruning/new/BufferedReader.readLine/0_sample-1_BufferedReader.readLine_graph_dump.txt'"
     ]
    }
   ],
   "source": [
    "file = \"/raid/cs21mtech12001/API-Misuse-Research/PDG-Gen/Repository/Processed Dataset/After pruning/new/BufferedReader.readLine/0_sample-1_BufferedReader.readLine_graph_dump.txt\"\n",
    "\n",
    "get_nodes_edges(file, add_reverse_edges = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to get CodeBERT embedding for nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4\"\n",
    "#device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "codebert_model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_embedding_from_codebert(nodes):\n",
    "    list_of_embeddings = []\n",
    "    for code_line in nodes.keys():\n",
    "        code_line = code_line.split(\"$$\")[1].strip()\n",
    "        code_tokens = tokenizer.tokenize(code_line)\n",
    "        tokens = [tokenizer.cls_token]+code_tokens+[tokenizer.eos_token]\n",
    "        tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        context_embeddings = codebert_model(torch.tensor(tokens_ids)[None,:])\n",
    "        cls_token_embedding = context_embeddings.last_hidden_state[0,0,:]\n",
    "        list_of_embeddings.append(cls_token_embedding)\n",
    "    return torch.stack(list_of_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to create data objects for GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tqdm\n",
    "import sys\n",
    "\n",
    "def create_graph_dataset(folders):\n",
    "  dataset =[]\n",
    "  for label, folder in tqdm.tqdm(enumerate(folders)):\n",
    "    print(\"\\nProcessing: {}\\n\".format(folder))\n",
    "    files = glob.glob(os.path.join(folder, '*.txt'))\n",
    "    print(\"\\nNumber of files: {}\\n\".format(len(files)))\n",
    "    count = 0\n",
    "    for file in files:\n",
    "\n",
    "      if(count % 5 == 0):\n",
    "        print(\"\\nAt file: {}\\n\".format(count))\n",
    "      nodes_dict, edge_indices_FD, edge_indices_CD, edge_indices, edge_type, file_name = get_nodes_edges(file, add_reverse_edges = True)\n",
    "      if(len(nodes_dict) == 0):\n",
    "        print(\"No Data: \", file)\n",
    "        continue\n",
    "      #print(nodes_dict, edge_indices_CD, edge_indices_FD, edge_type)\n",
    "\n",
    "      # Node feature matrix with shape [num_nodes, num_node_features]=(N, 768).\n",
    "      try:\n",
    "        CodeEmbedding = get_node_embedding_from_codebert(nodes_dict)\n",
    "      except Exception as e :\n",
    "        print(\"Error: \", e)\n",
    "        print(nodes_dict)\n",
    "        sys.exit()\n",
    "      #print(CodeEmbedding.shape)\n",
    "\n",
    "      # FIXING DATA FOTMATS AND SHAPE\n",
    "      x = torch.tensor(CodeEmbedding)\n",
    "      # print(x.shape)\n",
    "  \n",
    "      # data.y: Target to train against (may have arbitrary shape),\n",
    "      # graph-level targets of shape [1, *]\n",
    "      y = torch.tensor([label], dtype=torch.long)\n",
    "      #print(type(y))\n",
    "\n",
    "      # edge_index (LongTensor, optional) – Graph connectivity in COO format with shape [2, num_edges]\n",
    "      edge_index_CD = torch.tensor(edge_indices_CD, dtype=torch.long).t().contiguous()\n",
    "      edge_index_FD = torch.tensor(edge_indices_FD, dtype=torch.long).t().contiguous()\n",
    "      edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "      edge_type = torch.tensor(edge_type, dtype=torch.long).t().contiguous()\n",
    "      #print(edge_index_CD, edge_index_FD, edge_index, edge_type)\n",
    "  \n",
    "      data = Data(edge_index=edge_index, edge_type=edge_type, x=x)\n",
    "      data.y = y\n",
    "      data.num_nodes = len(nodes_dict)\n",
    "      data.api = file_name\n",
    "      dataset.append(data)\n",
    "      count += 1\n",
    "    \n",
    "  return dataset\n",
    "\n",
    "dataset = create_graph_dataset(folders)\n",
    "print(\"\\nLength of the dataset: \", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    torch.save(data, f'/raid/cs21mtech12001/API-Misuse-Research/PDG-Gen/Repository/Processed Dataset/Graph data/no_reverse_edge/{data.api}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from existing file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch \n",
    "\n",
    "dataset_folder_path = \"/raid/cs21mtech12001/API-Misuse-Research/PDG-Gen/Repository/Processed Dataset/Graph data/new\"\n",
    "graph_data_files = glob.glob(os.path.join(dataset_folder_path, '*.pt'))\n",
    "dataset = []\n",
    "for data_file in graph_data_files:\n",
    "    data = torch.load(data_file)\n",
    "    dataset.append(data)\n",
    "\n",
    "print(\"\\nLength of the dataset: \", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the datapoints if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLength of the dataset: \", len(dataset))\n",
    "filtered_dataset = []\n",
    "for data in dataset:\n",
    "    if data.num_nodes >=3 :\n",
    "        filtered_dataset.append(data)\n",
    "        \n",
    "print(\"\\nLength of the filtered dataset: \", len(filtered_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Statistics & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLength of the dataset: \", len(filtered_dataset))\n",
    "\n",
    "data = filtered_dataset[0]\n",
    "print(data, \"\\n\")\n",
    "print('=============================================================')\n",
    "\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "print(data.edge_index)\n",
    "print(data.edge_type)\n",
    "#print(data.x.dtype, data.edge_index.dtype, data.edge_type.dtype, data.y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "def visualize_graph(data, color):\n",
    "    G = to_networkx(data, to_undirected=False)\n",
    "    edge_labels = {}\n",
    "    for i in range(len(G.edges)):\n",
    "        edge = list(G.edges)[i]\n",
    "        edge_labels[edge[0], edge[1]] = int(data.edge_type[i])\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=True,\n",
    "                     node_color=color, cmap=\"Set2\")\n",
    "    nx.draw_networkx_edge_labels(G, pos=nx.spring_layout(G, seed=42), edge_labels=edge_labels, font_color='red')\n",
    "    plt.show()\n",
    "\n",
    "for G in filtered_dataset[30:35]:\n",
    "    visualize_graph(G, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label_counter = {\"PreparedStatement.executeQuery\" : 0, \"Calendar.getTime\": 0, \"URL.openConnection\": 0, \"BufferedReader.readLine\": 0}\n",
    "for data in filtered_dataset:\n",
    "    for label in Label_counter:\n",
    "        if label in data.api:\n",
    "            Label_counter[label] += 1\n",
    "            break\n",
    "print(\"API labels: \", Label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = random.sample(filtered_dataset, len(filtered_dataset))\n",
    "\n",
    "train_dataset = shuffled_dataset[:int(len(filtered_dataset) * 0.95)]\n",
    "test_dataset = shuffled_dataset[int(len(filtered_dataset) * 0.95):]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# for step, data in enumerate(train_loader):\n",
    "#     print(f'Step {step + 1}:')\n",
    "#     print('=======')\n",
    "#     print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "#     print(data)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_node_features = 768, num_classes = 4):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        graph_embedding = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(graph_embedding, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x, graph_embedding\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the GAT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_node_features = 768, num_classes = 4):\n",
    "        super(GAT, self).__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        self.conv1 = GATConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        graph_embedding = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(graph_embedding, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x, graph_embedding\n",
    "\n",
    "model = GAT(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "def visualize_embeddings(h, color):\n",
    "    z = umap.UMAP(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(hidden_channels=64)\n",
    "\n",
    "model.eval()\n",
    "dataloader = DataLoader(shuffled_dataset, len(shuffled_dataset), shuffle=False)\n",
    "for data in dataloader:\n",
    "    out, graph_embedding = model(data.x, data.edge_index, data.batch)\n",
    "    visualize_embeddings(graph_embedding, color=data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,10\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device, torch.cuda.device_count(), torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT(hidden_channels=64)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         data.to(device)\n",
    "         out, _ = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         #print(\"Out = {} and Label = {}\".format(out, data.y))\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "         total_loss += float(loss)\n",
    "    print(\"Loss: \", total_loss)\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         data.to(device)\n",
    "         out, _ = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(0, 5):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(torch.device(\"cpu\"))\n",
    "for data in DataLoader(shuffled_dataset, len(shuffled_dataset), shuffle=False):\n",
    "    out, graph_embedding = model(data.x, data.edge_index, data.batch)\n",
    "    visualize_embeddings(graph_embedding, color=data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/raid/cs21mtech12001/API-Misuse-Research/PDG-Gen/Repository/Saved_models/Graph_classification/GAT/model_with_reverse_edge.pt\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/raid/cs21mtech12001/API-Misuse-Research/PDG-Gen/Repository/Saved_models/Graph_classification/model_2.pt\"\n",
    "model = GCN(hidden_channels=64)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect embeddings for a specific class or API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "CLASS_LABEL = 2\n",
    "\n",
    "model.eval()\n",
    "train_api_embeddings, train_api_datapoints = [], []\n",
    "test_api_embeddings, test_api_datapoints = [], []\n",
    "\n",
    "for data in DataLoader(test_dataset, len(test_dataset), shuffle=False):\n",
    "    _, graph_embedding = model(data.x, data.edge_index, data.batch)\n",
    "    for i in range(len(test_dataset)):\n",
    "        if(int(data[i].y) == CLASS_LABEL):\n",
    "            test_api_datapoints.append(data[i])\n",
    "            test_api_embeddings.append(graph_embedding[i].detach().cpu().numpy())\n",
    "print(len(test_api_embeddings), len(test_api_datapoints))\n",
    "\n",
    "for data in DataLoader(train_dataset, len(train_dataset), shuffle=False):\n",
    "    _, graph_embedding = model(data.x, data.edge_index, data.batch)\n",
    "    for i in range(len(train_dataset)):\n",
    "        if(int(data[i].y) == CLASS_LABEL):\n",
    "            train_api_datapoints.append(data[i])\n",
    "            train_api_embeddings.append(graph_embedding[i].detach().cpu().numpy())\n",
    "print(len(train_api_embeddings), len(train_api_datapoints))\n",
    "\n",
    "total_api_embeddings, total_api_datapoints = train_api_embeddings + test_api_embeddings, train_api_datapoints + test_api_datapoints\n",
    "print(len(total_api_embeddings), len(total_api_datapoints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get optimal clusters number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_for_kmeans(embeddings):\n",
    "\n",
    "    inertias, no_of_clusters = [], list(range(1, len(embeddings)))\n",
    "\n",
    "    for i in no_of_clusters:\n",
    "        kmeans = KMeans(n_clusters=i)\n",
    "        kmeans.fit(embeddings)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "\n",
    "    plt.plot(no_of_clusters, inertias, marker='o')\n",
    "    plt.title('Elbow method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.show()\n",
    "    \n",
    "elbow_for_kmeans(train_api_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 10)\n",
    "kmeans.fit(train_api_embeddings)\n",
    "test_api_clusters_kmeans = kmeans.predict(test_api_embeddings)\n",
    "print(test_api_clusters_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize using t-SNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly_express as px\n",
    "import plotly.graph_objs as go\n",
    "import chart_studio.plotly as py\n",
    "import pandas as pd\n",
    "\n",
    "def visualize_clusters_using_tsne(vectors, clusters):\n",
    "\n",
    "    tsne_output_2d = TSNE(n_components = 2).fit_transform(vectors)\n",
    "    tsne_dataframe_2d = pd.DataFrame(tsne_output_2d)\n",
    "    tsne_dataframe_2d['cluster'] = clusters\n",
    "    tsne_dataframe_2d.columns = ['x1','x2','cluster']\n",
    "\n",
    "    tsne_output_3d = TSNE(n_components = 3).fit_transform(vectors)\n",
    "    tsne_dataframe_3d = pd.DataFrame(tsne_output_3d)\n",
    "    tsne_dataframe_3d['cluster'] = clusters\n",
    "    tsne_dataframe_3d.columns = ['x1','x2', 'x3', 'cluster']\n",
    "\n",
    "    fig = px.scatter(\n",
    "        tsne_dataframe_2d[[\"x1\", \"x2\"]], x=\"x1\", y=\"x2\",\n",
    "        color=tsne_dataframe_2d[\"cluster\"], labels={'color': 'clusters'}\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "        tsne_dataframe_3d[[\"x1\", \"x2\", \"x3\"]], x=\"x1\", y=\"x2\", z=\"x3\",\n",
    "        color=tsne_dataframe_3d[\"cluster\"], labels={'color': 'clusters'}\n",
    "    )\n",
    "    fig.update_traces(marker_size = 8)\n",
    "    fig.show()\n",
    "\n",
    "visualize_clusters_using_tsne(train_api_embeddings, kmeans.predict(train_api_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "\n",
    "def visualize_clusters_using_umap(vectors, clusters):\n",
    "\n",
    "    umap_2d = UMAP(n_components=2, init='random', random_state=0)\n",
    "    umap_output_2d = umap_2d.fit_transform(vectors)\n",
    "    umap_dataframe_2d = pd.DataFrame(umap_output_2d)\n",
    "    umap_dataframe_2d['cluster'] = clusters\n",
    "    umap_dataframe_2d.columns = ['x1','x2', 'cluster']\n",
    "\n",
    "    umap_3d = UMAP(n_components=3, init='random', random_state=0)\n",
    "    umap_output_3d = umap_3d.fit_transform(vectors)\n",
    "    umap_dataframe_3d = pd.DataFrame(umap_output_3d)\n",
    "    umap_dataframe_3d['cluster'] = clusters\n",
    "    umap_dataframe_3d.columns = ['x1','x2', 'x3', 'cluster']\n",
    "\n",
    "    fig_2d = px.scatter(\n",
    "        umap_dataframe_2d[[\"x1\", \"x2\"]], x=\"x1\", y=\"x2\",\n",
    "        color=umap_dataframe_2d[\"cluster\"], labels={'color': 'clusters'}\n",
    "    )\n",
    "    fig_3d = px.scatter_3d(\n",
    "        umap_dataframe_3d[[\"x1\", \"x2\", \"x3\"]], x=\"x1\", y=\"x2\", z=\"x3\",\n",
    "        color=umap_dataframe_3d[\"cluster\"], labels={'color': 'clusters'}\n",
    "    )\n",
    "    fig_3d.update_traces(marker_size=5)\n",
    "\n",
    "    fig_2d.show()\n",
    "    fig_3d.show()\n",
    "    \n",
    "visualize_clusters_using_umap(train_api_embeddings, kmeans.predict(train_api_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIRCH Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "\n",
    "model.eval()\n",
    "filtered_dataset_api_embeddings, filtered_dataset_api_datapoints = [], []\n",
    "\n",
    "for data in DataLoader(filtered_dataset, len(filtered_dataset), shuffle=False):\n",
    "    _, graph_embedding = model(data.x, data.edge_index, data.batch)\n",
    "    for i in range(len(filtered_dataset)):\n",
    "        filtered_dataset_api_datapoints.append(data[i])\n",
    "        filtered_dataset_api_embeddings.append(graph_embedding[i].detach().cpu().numpy())\n",
    "print(len(filtered_dataset_api_embeddings), len(filtered_dataset_api_datapoints))\n",
    "\n",
    "birch_model = Birch(n_clusters = None)\n",
    "filtered_dataset_clusters_birch = birch_model.fit_predict(train_api_embeddings)\n",
    "print(len(set(filtered_dataset_clusters_birch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "\n",
    "birch_model = Birch(n_clusters = None)\n",
    "train_api_clusters_birch = birch_model.fit_predict(train_api_embeddings)\n",
    "test_api_clusters_birch = birch_model.predict(test_api_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for i in train_api_clusters_birch:\n",
    "    if i in d:\n",
    "        d[i] += 1\n",
    "    else:\n",
    "        d[i] = 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_clusters_using_tsne(train_api_embeddings, train_api_clusters_birch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_clusters_using_umap(train_api_embeddings, train_api_clusters_birch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "train_api_clusters_dbscan = DBSCAN(eps=10, min_samples=4).fit_predict(train_api_embeddings)\n",
    "print(\"Clusters: {}\".format(train_api_clusters_dbscan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize using TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_clusters_using_tsne(train_api_embeddings, train_api_clusters_dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_clusters_using_umap(train_api_embeddings, train_api_clusters_dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Code-Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Code-Kernel cluster details for a particular API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAPI name: \", total_api_datapoints[0].api)\n",
    "\n",
    "code_kernel_clusters = {}\n",
    "code_kernel_api_cluster_mapping = {}\n",
    "\n",
    "for data in total_api_datapoints:\n",
    "    name_parts = data.api.split(\"_\")\n",
    "    file_name, cluster_no, api_name = name_parts[0].strip(), name_parts[1].strip(), name_parts[2].strip()\n",
    "    if cluster_no in code_kernel_clusters:\n",
    "        code_kernel_clusters[cluster_no].append(data.api)\n",
    "    else:\n",
    "        code_kernel_clusters[cluster_no] = [data.api]\n",
    "    code_kernel_api_cluster_mapping[data.api] = cluster_no\n",
    "        \n",
    "print(\"\\nTotal data points: \", len(code_kernel_api_cluster_mapping))\n",
    "print(\"\\nNumber of cluster in Code-Kernel: \", len(code_kernel_clusters))\n",
    "for cluster_no in code_kernel_clusters:\n",
    "    print(\"{} : {}\".format(cluster_no, len(code_kernel_clusters[cluster_no])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cluster-classification ground-truth from Code-Kernel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_classification_from_clustering(data_to_cluster_map):\n",
    "    classification = {}\n",
    "    data_points = list(data_to_cluster_map.keys())\n",
    "    for i in range(len(data_points)):\n",
    "        j = i+1\n",
    "        while j < len(data_points):\n",
    "            classification[tuple([data_points[i], data_points[j]])] = 1 if data_to_cluster_map[data_points[i]] == data_to_cluster_map[data_points[j]] else 0\n",
    "            j += 1\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_kernel_classification = do_classification_from_clustering(code_kernel_api_cluster_mapping)\n",
    "print(\"\\nTotal code-kernel classifications : \", len(code_kernel_classification))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster GNN embeddings with same cluster numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "\n",
    "birch_model = Birch(n_clusters = 17)\n",
    "total_api_clusters_birch = birch_model.fit_predict(total_api_embeddings)\n",
    "\n",
    "total_api_cluster_mapping = {}\n",
    "for data, cluster in zip(total_api_datapoints, total_api_clusters_birch):\n",
    "    total_api_cluster_mapping[data.api] = cluster\n",
    "    \n",
    "print(\"\\nTotal data points: \", len(total_api_cluster_mapping))\n",
    "total_api_classification = do_classification_from_clustering(total_api_cluster_mapping)\n",
    "print(\"\\nTotal GNN classifications : \", len(total_api_classification))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare GNN clustering accuracy wrt Code-Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "code_kernel_labels, GNN_labels = [], []\n",
    "for pair in total_api_classification:\n",
    "    code_kernel_labels.append(code_kernel_classification[pair])\n",
    "    GNN_labels.append(total_api_classification[pair])\n",
    "    \n",
    "print(\"\\nConfusion Matrix: \", confusion_matrix(code_kernel_labels, GNN_labels))\n",
    "print(\"\\nAccuracy: \", accuracy_score(code_kernel_labels, GNN_labels) * 100)\n",
    "print(\"\\nF1-Score: \", f1_score(code_kernel_labels, GNN_labels) * 100)\n",
    "print(\"\\nPrecision: \", precision_score(code_kernel_labels, GNN_labels) * 100)\n",
    "print(\"\\nRecall: \", recall_score(code_kernel_labels, GNN_labels) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some wrongly classified datapoints for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Code-Kernel using manually labelled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tqdm\n",
    "import sys\n",
    "\n",
    "folder_path = \"/raid/cs21mtech12001/API-Misuse-Research/PDG-Gen/Repository/CodeKernel_Manual_Data/Processed_data/after_pruning/new\"\n",
    "pdg_folders_list = glob.glob(folder_path + \"/*/\")\n",
    "print(\"\\nNumber of total APIs: {}\\n\".format(len(pdg_folders_list)))\n",
    "\n",
    "manual_dataset = create_graph_dataset(pdg_folders_list)\n",
    "print(\"\\nLength of the manually labelled dataset: \", len(manual_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in manual_dataset:\n",
    "    torch.save(data, f'/raid/cs21mtech12001/API-Misuse-Research/PDG-Gen/Repository/CodeKernel_Manual_Data/Processed_data/graph_data/no_reverse_edge/{data.api}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLength of the manual dataset: \", len(manual_dataset))\n",
    "filtered_manual_dataset = []\n",
    "for data in manual_dataset:\n",
    "    if data.num_nodes >= 3 :\n",
    "        filtered_manual_dataset.append(data)\n",
    "        \n",
    "print(\"\\nLength of the filtered manual dataset: \", len(filtered_manual_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLength of the dataset: \", len(filtered_manual_dataset))\n",
    "\n",
    "data = filtered_manual_dataset[0]\n",
    "print(data, \"\\n\")\n",
    "print('=============================================================')\n",
    "\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "print(data.edge_index)\n",
    "print(data.edge_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch \n",
    "\n",
    "dataset_folder_path = \"/raid/cs21mtech12001/API-Misuse-Research/PDG-Gen/Repository/CodeKernel_Manual_Data/Processed_data/graph_data/new/\"\n",
    "graph_data_files = glob.glob(os.path.join(dataset_folder_path, '*.pt'))\n",
    "manual_dataset = []\n",
    "for data_file in graph_data_files:\n",
    "    data = torch.load(data_file)\n",
    "    manual_dataset.append(data)\n",
    "\n",
    "print(\"\\nLength of the manual dataset: \", len(manual_dataset))\n",
    "filtered_manual_dataset = []\n",
    "for data in manual_dataset:\n",
    "    if data.num_nodes >= 3 :\n",
    "        filtered_manual_dataset.append(data)\n",
    "        \n",
    "print(\"\\nLength of the filtered manual dataset: \", len(filtered_manual_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "manual_data_embeddings = {}\n",
    "\n",
    "for data in DataLoader(manual_dataset, len(manual_dataset), shuffle=False):\n",
    "    _, graph_embedding = model(data.x, data.edge_index, data.batch)\n",
    "    for i in range(len(manual_dataset)):\n",
    "        manual_data_embeddings[data[i].api] = graph_embedding[i].detach().cpu().numpy()\n",
    "    \n",
    "print(len(manual_data_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get cluster labels for Ground-truth, Code-Kernel and GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "APIs = [\"Driver.connect\", \"FileUtils.writeStringToFile\", \"Graphics2D.fill\", \"IOUtils.toString\", \"PrinterJob.pageDialog\", \"Properties.loadFromXML\", \n",
    "        \"Servant._poa\", \"Window.pack\"]\n",
    "\n",
    "gt_labels_for_manual_data = dict.fromkeys(APIs, 0)\n",
    "ck_labels_for_manual_data = dict.fromkeys(APIs, 0)\n",
    "gnn_labels_for_manual_data = dict.fromkeys(APIs, 0)\n",
    "\n",
    "for api in APIs:\n",
    "    gt_labels_for_manual_data[api] = copy.deepcopy(dict({}))\n",
    "    ck_labels_for_manual_data[api] = copy.deepcopy(dict({}))\n",
    "    gnn_labels_for_manual_data[api] = copy.deepcopy(dict({}))\n",
    "\n",
    "for data in manual_dataset:\n",
    "    #print(data.api)\n",
    "    parts = data.api.strip().split(\"_\")\n",
    "    ck_label, gt_label = -1, -1\n",
    "    flag = False\n",
    "    for i in range(len(parts)):\n",
    "        if \"sample\" in parts[i]:\n",
    "            gt_label = int(parts[i].split(\"-\")[1])\n",
    "            try:\n",
    "                ck_label = int(parts[i-1])\n",
    "            except ValueError:\n",
    "                print(\"\\nValue error for : \", data.api)\n",
    "                ck_label = 1\n",
    "            flag = True\n",
    "            break\n",
    "    if not flag:\n",
    "        print(\"\\nck/gt label not found\")\n",
    "    flag = False\n",
    "    for api in APIs:\n",
    "        if api in data.api:\n",
    "            #print(api, data.api)\n",
    "            gt_labels_for_manual_data[api][data.api] = gt_label\n",
    "            ck_labels_for_manual_data[api][data.api] = ck_label\n",
    "            gnn_labels_for_manual_data[api][data.api] = -1\n",
    "            flag = True            \n",
    "            break\n",
    "    if not flag:\n",
    "        print(\"\\nAPI label is not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_points_gt, total_points_ck, total_points_gnn = 0, 0, 0\n",
    "\n",
    "for api in gt_labels_for_manual_data:\n",
    " total_points_gt += len(gt_labels_for_manual_data[api])\n",
    " total_points_ck += len(ck_labels_for_manual_data[api])\n",
    " total_points_gnn += len(gnn_labels_for_manual_data[api])\n",
    " \n",
    "print(total_points_gt, total_points_ck, total_points_gnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster using BIRCH clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for api in gnn_labels_for_manual_data.keys():\n",
    "    embeddings_as_list, file_names = [], []\n",
    "    for file_name in gnn_labels_for_manual_data[api].keys():\n",
    "        embeddings_as_list.append(manual_data_embeddings[file_name])\n",
    "        file_names.append(file_name)\n",
    "        \n",
    "    if len(embeddings_as_list) == 0:\n",
    "        print(\"\\nNo embeddings for api: \", api)\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        birch_silhouette_scores= []\n",
    "        no_of_clusters = list(range(2, len(embeddings_as_list)))\n",
    "        for n_cluster in no_of_clusters:\n",
    "            birch_silhouette_scores.append(silhouette_score(embeddings_as_list, Birch(n_clusters = n_cluster).fit_predict(embeddings_as_list)))\n",
    "    \n",
    "        correct_cluster_number, max_silhouette_scores = -1, -2\n",
    "        for cluster_no, birch_silhouette_score in zip(no_of_clusters, birch_silhouette_scores):\n",
    "            correct_cluster_number = cluster_no if max_silhouette_scores < birch_silhouette_score else correct_cluster_number\n",
    "            max_silhouette_scores = birch_silhouette_score if max_silhouette_scores < birch_silhouette_score else max_silhouette_scores\n",
    "        #print(\"\\nMax Silhouette score for {} is {} for clusters {}\".format(api, max_silhouette_scores, correct_cluster_number))\n",
    "    except Exception as e:\n",
    "        print(\"\\nSilhouette error for API {} : {}\".format(api, e))\n",
    "        correct_cluster_number = None\n",
    "    \n",
    "    #correct_cluster_number = None\n",
    "    birch_model = Birch(n_clusters = correct_cluster_number)\n",
    "    clusters_birch = birch_model.fit_predict(embeddings_as_list)\n",
    "    print(\"\\nNumber of clusters for {} is {} having total {} datapoints\".format(api, len(set(clusters_birch)), len(embeddings_as_list)))\n",
    "\n",
    "    for file_name, cluster_no in zip(file_names, clusters_birch):\n",
    "        gnn_labels_for_manual_data[api][file_name] = cluster_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for api in gnn_labels_for_manual_data.keys():\n",
    "    for file_name in gnn_labels_for_manual_data[api].keys():\n",
    "        if gnn_labels_for_manual_data[api][file_name] == -1:\n",
    "            print(\"\\nLabel not set for : \", file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster using KMeans cluatering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for api in gnn_labels_for_manual_data.keys():\n",
    "    embeddings_as_list, file_names = [], []\n",
    "    for file_name in gnn_labels_for_manual_data[api].keys():\n",
    "        embeddings_as_list.append(manual_data_embeddings[file_name])\n",
    "        file_names.append(file_name)\n",
    "        \n",
    "    print(\"\\nFor \", api)\n",
    "    \n",
    "    elbow_for_kmeans(embeddings_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_cluster_numbers = {\"Driver.connect\" : 5, \"FileUtils.writeStringToFile\" : 2, \n",
    "                          \"Graphics2D.fill\" : 5, \"IOUtils.toString\" : 2, \n",
    "                          \"PrinterJob.pageDialog\" : 8, \"Properties.loadFromXML\" : 6, \n",
    "                          \"Servant._poa\" : 4, \"Window.pack\" : 3}\n",
    "\n",
    "for api in gnn_labels_for_manual_data.keys():\n",
    "    embeddings_as_list, file_names = [], []\n",
    "    for file_name in gnn_labels_for_manual_data[api].keys():\n",
    "        embeddings_as_list.append(manual_data_embeddings[file_name])\n",
    "        file_names.append(file_name)\n",
    "        \n",
    "    kmeans_model = KMeans(n_clusters = KMeans_cluster_numbers[api])\n",
    "    clusters_kmeans = kmeans_model.fit_predict(embeddings_as_list)\n",
    "    print(\"\\nNumber of clusters for {} is {} having total {} datapoints\".format(api, len(set(clusters_kmeans)), len(embeddings_as_list)))\n",
    "\n",
    "    for file_name, cluster_no in zip(file_names, clusters_kmeans):\n",
    "        gnn_labels_for_manual_data[api][file_name] = cluster_no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair-wise classification labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_classification_labels, ck_classification_labels, gnn_classification_labels = {}, {}, {}\n",
    "\n",
    "for api in APIs:\n",
    "    gt_classification_labels[api] = do_classification_from_clustering(gt_labels_for_manual_data[api])\n",
    "    ck_classification_labels[api] = do_classification_from_clustering(ck_labels_for_manual_data[api])\n",
    "    gnn_classification_labels[api] = do_classification_from_clustering(gnn_labels_for_manual_data[api])\n",
    "    \n",
    "    print(\"\\n\", len(gt_labels_for_manual_data[api]), len(gt_classification_labels[api]), \n",
    "          len(ck_labels_for_manual_data[api]), len(ck_classification_labels[api]),\n",
    "          len(gnn_labels_for_manual_data[api]), len(gnn_classification_labels[api]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification accuracy for Code-Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score,classification_report\n",
    "\n",
    "for api in APIs:\n",
    "    \n",
    "    ck_classification_labels_as_list, gt_classification_labels_as_list = [], []\n",
    "    \n",
    "    for pair in gt_classification_labels[api]:\n",
    "        gt_classification_labels_as_list.append(gt_classification_labels[api][pair])\n",
    "        ck_classification_labels_as_list.append(ck_classification_labels[api][pair])\n",
    "    \n",
    "    Confusion_matrix = confusion_matrix(gt_classification_labels_as_list, ck_classification_labels_as_list)\n",
    "    accuracy = \"{:.2f}\".format(accuracy_score(gt_classification_labels_as_list, ck_classification_labels_as_list) * 100)\n",
    "    precision = \"{:.2f}\".format(precision_score(gt_classification_labels_as_list, ck_classification_labels_as_list) * 100)\n",
    "    recall = \"{:.2f}\".format(recall_score(gt_classification_labels_as_list, ck_classification_labels_as_list) * 100)\n",
    "    F1_score = \"{:.2f}\".format(f1_score(gt_classification_labels_as_list, ck_classification_labels_as_list) * 100)\n",
    "    \n",
    "    print(\"\\nAccuracy for API: \", api)\n",
    "    print(\"Accuracy: {}, precision: {}, recall: {} and f1-score: {}\".format(accuracy, precision, recall, F1_score))\n",
    "    # if api == \"IOUtils.toString\":\n",
    "    #     print(gt_classification_labels_as_list, ck_classification_labels_as_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification accuracy for GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "for api in APIs:\n",
    "    \n",
    "    gt_classification_labels_as_list, gnn_classification_labels_as_list = [], []\n",
    "    for pair in gt_classification_labels[api]:\n",
    "        gt_classification_labels_as_list.append(gt_classification_labels[api][pair])\n",
    "        gnn_classification_labels_as_list.append(gnn_classification_labels[api][pair])\n",
    "        \n",
    "    Confusion_matrix = confusion_matrix(gt_classification_labels_as_list, gnn_classification_labels_as_list)\n",
    "    accuracy = \"{:.2f}\".format(accuracy_score(gt_classification_labels_as_list, gnn_classification_labels_as_list) * 100)\n",
    "    precision = \"{:.2f}\".format(precision_score(gt_classification_labels_as_list, gnn_classification_labels_as_list) * 100)\n",
    "    recall = \"{:.2f}\".format(recall_score(gt_classification_labels_as_list, gnn_classification_labels_as_list) * 100)\n",
    "    F1_score = \"{:.2f}\".format(f1_score(gt_classification_labels_as_list, gnn_classification_labels_as_list) * 100)\n",
    "    \n",
    "    print(\"\\nAccuracy for API: \", api)\n",
    "    print(\"Accuracy: {}, precision: {}, recall: {} and f1-score: {}\".format(accuracy, precision, recall, F1_score))\n",
    "    \n",
    "    if api == \"FileUtils.writeStringToFile\":\n",
    "        print(gt_classification_labels_as_list, gnn_classification_labels_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for api in gt_labels_for_manual_data.keys():\n",
    "    if api != \"Driver.connect\":\n",
    "        continue\n",
    "    #print(gt_labels_for_manual_data[api])\n",
    "    clusters = {}\n",
    "    for api_example in gt_labels_for_manual_data[api].keys():\n",
    "        if gt_labels_for_manual_data[api][api_example] in clusters:\n",
    "            clusters[gt_labels_for_manual_data[api][api_example]].append(api_example)\n",
    "        else:\n",
    "            clusters[gt_labels_for_manual_data[api][api_example]] = [api_example]\n",
    "    print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for api in gnn_labels_for_manual_data.keys():\n",
    "    if api != \"Driver.connect\":\n",
    "        continue\n",
    "    #print(gt_labels_for_manual_data[api])\n",
    "    clusters = {}\n",
    "    for api_example in gnn_labels_for_manual_data[api].keys():\n",
    "        if gnn_labels_for_manual_data[api][api_example] in clusters:\n",
    "            clusters[gnn_labels_for_manual_data[api][api_example]].append(api_example)\n",
    "        else:\n",
    "            clusters[gnn_labels_for_manual_data[api][api_example]] = [api_example]\n",
    "    print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for api in gnn_labels_for_manual_data.keys():\n",
    "    if api != \"Driver.connect\":\n",
    "        continue\n",
    "    #print(gt_labels_for_manual_data[api])\n",
    "    clusters = {}\n",
    "    for api_example in gnn_labels_for_manual_data[api].keys():\n",
    "        if \"sample-2_\" in api_example:\n",
    "            if gnn_labels_for_manual_data[api][api_example] in clusters:\n",
    "                clusters[gnn_labels_for_manual_data[api][api_example]].append(api_example)\n",
    "            else:\n",
    "                clusters[gnn_labels_for_manual_data[api][api_example]] = [api_example]\n",
    "    print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_examples = ['M_connect_131_ConnectionManager_0_0_sample-0_Driver.connect_graph_dump.txt', 'M_connect_132_ConnectionManager_0_0_sample-0_Driver.connect_graph_dump.txt', 'M_connect_114_SQLGet_0_0_sample-0_Driver.connect_graph_dump.txt']\n",
    "for data in manual_dataset:\n",
    "    if data.api in api_examples:\n",
    "        visualize_graph(data, color=\"blue\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
